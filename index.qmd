---
title: "Tone Matters: Sentiment Classification of Support Tweets Using VADER and XGBoost"
subtitle: ""
author: "Samantha Chickeletti & Michael Alfrey (Advisor: Dr. Cohen)"
date: "`r Sys.Date()`"
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)


## Introduction

In today‚Äôs digital landscape, customer support conversations increasingly take place over chat and social media platforms. These short-form exchanges are often emotionally charged and can signal a customer‚Äôs satisfaction, frustration, or potential escalation. Understanding the emotional tone behind these messages is critical for improving service quality, anticipating customer needs, and enhancing the overall customer experience. Yet, analyzing this kind of shorthand-heavy language presents a unique challenge for traditional sentiment analysis models.

This project explores how VADER (Valence Aware Dictionary for sEntiment Reasoning), a lexicon-based sentiment analysis tool, can classify tone in real customer support messages [@hutto2014vader]. Its design prioritizes speed and interpretability, making it ideal for short, informal content like tweets and chat messages. VADER‚Äôs scoring mechanism is particularly sensitive to social media features such as emojis, capitalization, and punctuation, which are often critical to conveying tone in these environments [@barik2024vader].

To build a full machine learning pipeline around VADER, we will use its sentiment scores (positive, neutral, negative) as labels and train an XGBoost classifier using TF-IDF features extracted from the message text. XGBoost is well-suited for this task because it performs efficiently with sparse, high-dimensional data and eliminates the need to hand-label messages or train a separate sentiment model from scratch.

The dataset selected for this project is the ‚ÄúCustomer Support on Twitter‚Äù dataset from Kaggle, which contains real-world support interactions between users and brands such as Apple, Amazon, and Comcast. The messages are short, informal, and emotionally expressive‚Äîclosely mirroring real-world customer support scenarios‚Äîand make the dataset ideal for sentiment analysis and predictive modeling.

#### Literature Review 

Natural Language Processing (NLP) has become a vital tool for understanding customer sentiment across digital platforms. A variety of approaches have been proposed in the literature, from lexicon-based models such as VADER to machine learning methods like XGBoost. This review highlights the studies that informed the methodological design of our project.

#### Lexicon-Based Sentiment Analysis

The foundation of our sentiment scoring approach is VADER, a rule-based model that excels at detecting sentiment in informal, short-form text such as tweets and chat messages [@hutto2014vader]. VADER‚Äôs robustness to capitalization, punctuation, and emoji usage makes it particularly well-suited for analyzing customer service conversations on social media.

Recent research continues to support and expand on VADER's use. Barik and Misra [@barik2024vader] evaluated an improved VADER lexicon in analyzing e-commerce reviews and emphasized its interpretability and processing speed. Chadha and Aryan [@chadha2023vader] also confirmed VADER‚Äôs reliability in sentiment classification tasks, noting its effectiveness in fast-paced business contexts. Youvan [@youvan2024vader] offered a comprehensive review of VADER‚Äôs core logic, highlighting its treatment of intensifiers, negations, and informal expressions.

#### Machine Learning for Sentiment Classification

To complement VADER‚Äôs labeling, we incorporate XGBoost, an efficient and scalable gradient boosting algorithm, as a supervised classifier. Lestari et al. [@lestari2025xgboost] compared XGBoost with AdaBoost for movie review classification and found XGBoost achieved higher accuracy and generalizability. Sefara and Rangata [@sefara2024domain] also found XGBoost to be the most effective model for classifying domain-specific tweets, outperforming Logistic Regression and SVM in both performance and efficiency. Lu and Schelle [@lu2025tesla] demonstrated how XGBoost could be used to extract interpretable feature importance from tweet sentiment, providing additional value for insights and decision-making.


## Methods

#### VADER

To assign sentiment labels to customer support tweets, we used VADER (Valence Aware Dictionary and sEntiment Reasoner). A lexicon and rule-based model specifically built for short, informal, and social media-style texts [@hutto2014vader]. VADER excels in this because it was developed as a curated sentiment lexicon that includes not only standard vocabulary, but also emoticons, acronyms (e.g., ‚ÄúLOL‚Äù, ‚ÄúROFL‚Äù), slang (e.g., ‚Äúmeh‚Äù, ‚Äúsux‚Äù), and even emojis. These elements are essential for analyzing customer support chats on platforms like Twitter, where users often express emotional tone in unconventional ways [@chadha2023vader]. Unlike deep learning models, VADER is lightweight and interpretable. It doesn‚Äôt require extensive training data or high computational resources, making it ideal for rapid deployment in real-world, high-volume contexts such as customer support. Instead, it applies a set of grammatical and syntactical rules to adjust sentiment intensity scores. For instance, it increases sentiment weight when all-caps or exclamation points are used in conjunction with a recognized word (e.g., ‚ÄúTERRIBLE!!‚Äù) and modifies polarity for phrases that include negations (e.g., ‚Äúnot bad‚Äù is interpreted more positively than ‚Äúbad‚Äù) [@barik2024ivader]. For example, consider the tweet: "I waited 40 minutes and STILL no reply. This is unacceptable!".  VADER flags this as strong negative sentiment in this message due to the all-caps ‚ÄúSTILL‚Äù the emphasis of punctuation, and the inclusion of the word ‚Äúunacceptable,‚Äù which carries a high negative valence in its lexicon. The compound score for this message might fall around ‚Äì0.75, classifying it as negative. In contrast, a tweet like ‚ÄúThanks so much for your help üòä‚Äù would receive a high positive score, boosted by the appreciative tone and the smiling emoji. Each text is ultimately scored on three dimensions (positive, neutral, and negative) and combined into a compound score ranging from -1 (extremely negative) to +1 (extremely positive). Based on thresholds established by Hutto and Gilbert (2014), we define sentiment categories as:	Positive: compound score ‚â• 0.05, Neutral: -0.05 < compound score < 0.05, Negative: compound score ‚â§ -0.05. These thresholds have proven effective in past research. For instance, Gandy et al. (2025) evaluated VADER‚Äôs performance on public health discussions and found it achieved a 74% accuracy rate, with high sensitivity for detecting negative sentiment. A key concern in our project, where escalated or frustrated messages from customers should be captured accurately [@gandy2025public]. Similarly, Chadha and Chaudhary (2023) demonstrated how VADER interprets subtle linguistic cues, even in messages that lack full grammar or use emojis as emotional shorthand, making it well-matched to our Twitter-based dataset [@chadha2023vader]. By relying on VADER to label tweets as positive, neutral, or negative without manual annotation, we create a foundation for downstream supervised learning. This aligns with findings from Lu (2025), who showed that VADER-labeled tweets combined with TF-IDF and XGBoost models yielded results comparable to manually labeled data [@lu2025tesla].

#### TF-IDF 

After labeling each tweet with a sentiment category using VADER, the next step is to prepare the message text for machine learning. To do this, we convert the raw text messages into numerical features using Term Frequency‚ÄìInverse Document Frequency (TF-IDF). This step will be essential to enabling the XGBoost classifier to process textual input. TF-IDF is a widely used statistical tool capable of capturing the importance of words relative to both the individual message and the larger corpus. It is particularly effective for datasets like ours: short-form, informal, and sparse in structure. Tweets in customer support often include abbreviations, slang, or repetition, and TF-IDF helps retain the meaningful patterns without overwhelming the classifier with irrelevant frequently used words. In our pipeline, this vectorization bridges the gap between qualitative scoring of VADER with the quantitative input required for supervised learning. Studies have shown that combining TF-IDF with tree-based models yields strong results in sentiment analysis: Lestari et al. (2025) found this combination improved accuracy and generalizability across multiple datasets, while Sefara and Rangata (2024) demonstrated its effectiveness for classifying domain-specific tweets [@lestari2025xgboost; @sefara2024domain]. By aligning the language structure with statistical weight, TF-IDF enhances the model's ability to prioritize the most discriminative features for downstream classification.

#### XGBoost 

With tweets now represented as TF-IDF vectors and labeled with sentiment scores from VADER, we train a XGBoost (Extreme Gradient Boosting) classifier to detect patterns in tone and emotion across messages. XGBoost is a powerful, scalable tree-based ensemble learning algorithm that builds decision trees sequentially. It optimizes performance by minimizing a loss function over each iteration. This classifier's ability to handle high-dimensional, sparse input like TF-IDF vectors makes it an excellent fit for our dataset. By learning from VADER‚Äôs labeled examples, XGBoost will extend the sentiment classification process beyond just lexicon rules and will enable it to be generalized to unseen or new messages. For example, while VADER may recognize ‚Äúunacceptable‚Äù as a negative word, XGBoost may learn that the phrase ‚Äústill no reply‚Äù often occurs in complaints and weight it accordingly even if it‚Äôs less emotionally charged on its own.
This predictive layer is what supports scalable automation and could serve in the future as the foundation of real-time sentiment monitoring or analysis of escalation risks. Prior research highlights XGBoost‚Äôs effectiveness in similar NLP pipelines, particularly when combined with TF-IDF to balance performance and interpretability [@lestari2025xgboost; @sefara2024domain; @lu2025tesla]. Together, TF-IDF and XGBoost turn VADER‚Äôs interpretability into actionability. This layered approach positions our model not only as a tool for retrospective analysis, but as a scalable framework for monitoring customer sentiment in fast-paced service environments.

#### Evaluation Metrics

Once the sentiment labels are used to train a classification model, it becomes essential to evaluate how effectively the model performs on new, unseen data. In this project, we use four standard evaluation metrics; accuracy, precision, recall, and F1 score to assess model performance. These metrics are particularly meaningful in sentiment analysis where class imbalances can skew basic interpretations of model success.
Accuracy measures the overall proportion of correctly predicted sentiment labels. While widely used, accuracy can be misleading when one class dominates the dataset. For instance, if most customer tweets are neutral, a model predicting "neutral" for everything may appear accurate without being useful. Gandy et al. (2025) encountered a similar imbalance in their YouTube sentiment classification, where negative sentiment detection was more crucial than overall accuracy alone [@gandy2025public].
Precision quantifies the percentage of correct predictions among all tweets predicted to be in a given class. In our context, this metric is particularly important for negative sentiment. Misclassifying a clearly frustrated customer as ‚Äúneutral‚Äù could delay issue resolution and damage brand perception. Chadha and Chaudhary (2023) emphasized the importance of precision when analyzing short-form user interactions, noting that social media sentiment often requires extra sensitivity to context and punctuation [@chadha2023vader].
Recall tells us how many true instances of a particular sentiment class were correctly identified. For example, high recall for the negative class ensures we‚Äôre capturing the bulk of dissatisfied customers. In service environments where escalation matters, this becomes critical. Barik et al. (2024) and Lu (2025) both highlighted that VADER-augmented models often produce better recall for negative tone detection, particularly when paired with interpretable classifiers like XGBoost [@barik2024ivader; @lu2025tesla].
F1 Score is the harmonic mean of precision and recall. It serves as a balanced metric in cases where both false positives and false negatives carry meaningful consequences. In the context of chat-based customer service, the F1 score reflects how reliably the model can flag messages that require escalation while avoiding over-alerting on neutral or benign sentiment.
Together, these evaluation metrics provide the framework for assessing the predictive power and reliability of our model. They allow us to go beyond surface level accuracy and instead focus on the model's ability to support real-time decision-making in digital customer service environments.

Note: Some parts of this project were assisted by ChatGPT for writing support and citation formatting. All content was reviewed and edited by the authors to ensure accuracy and originality.


## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
