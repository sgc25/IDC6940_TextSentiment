---
title: "Tone Matters: Sentiment Classification of Support Tweets Using VADER and XGBoost"
subtitle: ""
author: "Samantha Chickeletti & Michael Alfrey (Advisor: Dr. Cohen)"
date: "`r Sys.Date()`"
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction

In today’s digital landscape, customer support conversations increasingly take place over chat and social media platforms. These short-form exchanges are often emotionally charged and can signal a customer’s satisfaction, frustration, or potential escalation. Understanding the emotional tone behind these messages is critical for improving service quality, anticipating customer needs, and enhancing the overall customer experience. Yet, analyzing this kind of shorthand-heavy language presents a unique challenge for traditional sentiment analysis models.

This project explores how VADER (Valence Aware Dictionary for sEntiment Reasoning), a lexicon and rule-based sentiment analysis tool, can be used to classify the tone of real customer support messages [@hutto2014vader]. Unlike deep learning models, VADER is lightweight, interpretable, and tuned for informal language like that found in chats and social media channels. It captures nuances in language through lexical scoring that accounts for punctuation, capitalization, and emojis, making it especially suited to these use cases [@barik2024vader].

To build a full machine learning pipeline around VADER, we will use its sentiment scores (positive, neutral, negative) as labels and train an XGBoost classifier using TF-IDF features extracted from the message text. XGBoost is well-suited for this task because it performs efficiently with sparse, high-dimensional data and eliminates the need to hand-label messages or train a separate sentiment model from scratch.

The dataset selected for this project is the “Customer Support on Twitter” dataset from Kaggle, which contains real-world support interactions between users and brands such as Apple, Amazon, and Comcast. The messages are short, informal, and emotionally expressive—closely mirroring real-world customer support scenarios—and make the dataset ideal for sentiment analysis and predictive modeling.

### Literature Review

Natural Language Processing (NLP) has become a vital tool for understanding customer sentiment across digital platforms. A variety of approaches have been proposed in the literature, from lexicon-based models such as VADER to machine learning methods like XGBoost. This review highlights the studies that informed the methodological design of our project.

### Lexicon-Based Sentiment Analysis

The foundation of our sentiment scoring approach is VADER, a rule-based model that excels at detecting sentiment in informal, short-form text such as tweets and chat messages [@hutto2014vader]. VADER’s robustness to capitalization, punctuation, and emoji usage makes it particularly well-suited for analyzing customer service conversations on social media.

Recent research continues to support and expand on VADER's use. Barik and Misra [@barik2024vader] evaluated an improved VADER lexicon in analyzing e-commerce reviews and emphasized its interpretability and processing speed. Chadha and Aryan [@chadha2023vader] also confirmed VADER’s reliability in sentiment classification tasks, noting its effectiveness in fast-paced business contexts. Youvan [@youvan2024vader] offered a comprehensive review of VADER’s core logic, highlighting its treatment of intensifiers, negations, and informal expressions.

### Machine Learning for Sentiment Classification

To complement VADER’s labeling, we incorporate XGBoost, an efficient and scalable gradient boosting algorithm, as a supervised classifier. Lestari et al. [@lestari2025xgboost] compared XGBoost with AdaBoost for movie review classification and found XGBoost achieved higher accuracy and generalizability. Sefara and Rangata [@sefara2024domain] also found XGBoost to be the most effective model for classifying domain-specific tweets, outperforming Logistic Regression and SVM in both performance and efficiency. Lu and Schelle [@lu2025tesla] demonstrated how XGBoost could be used to extract interpretable feature importance from tweet sentiment, providing additional value for insights and decision-making.


## Methods

Some parts of this project were assisted by ChatGPT for writing support and citation formatting. Everything was reviewed and edited by the authors.


### VADER


### TF-IDF

To bridge the gap between VADER’s sentiment labels and the supervised learning model we plan to implement, we will transform the raw text messages into numerical features using Term Frequency–Inverse Document Frequency (TF-IDF). This step will be essential to enabling the XGBoost classifier to process textual input. TF-IDF is a widely used statistical tool capable of capturing the importance of words relative to both the individual message and the larger corpus. It is particularly effective for high-dimensional and sparse data, just like the short, informal messages found in customer support interactions dataset. In the context of sentiment classification, TF-IDF will help the model focus on the most informative terms by assigning lower weights to common words and higher weights to phrases that are more discriminative. Studies have shown that combining TF-IDF with tree-based models yields strong results in sentiment analysis: Lestari et al. (2025) found this combination improved accuracy and generalizability across multiple datasets, while Sefara and Rangata (2024) demonstrated its effectiveness for classifying domain-specific tweets [@lestari2025xgboost; @sefara2024domain].

### XGBoost

Following the feature extraction with TF-IDF, we will train an XGBoost (Extreme Gradient Boosting) classifier to learn patterns associated with the sentiment labels generated by VADER. XGBoost is a powerful, scalable tree-based ensemble learning algorithm that builds decision trees sequentially. It optimizes performance by minimizing a loss function over each iteration. This classifier's ability to handle high-dimensional, sparse input like TF-IDF vectors makes it an excellent fit for our dataset. By learning from VADER’s labeled examples, XGBoost will extend the sentiment classification process beyond just lexicon rules, and will enable it to be generalized to unseen or new messages. This predictive layer is what supports scalable automation and could serve in the future as the foundation of real-time sentiment monitoring or analysis of escalation risks. Prior research highlights XGBoost’s effectiveness in similar NLP pipelines, particularly when combined with TF-IDF to balance performance and interpretability [@lestari2025xgboost; @sefara2024domain; @lu2025tesla].

### Evaluation Metrics



## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
